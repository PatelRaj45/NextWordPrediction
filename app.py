# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Nextâ€‘Word Predictor Â· GRU (100Â epochs, no EarlyÂ Stopping)
#  Streamlit UI polish: wide layout, sidebar info, example picker, result card
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np, pickle, textwrap
from pathlib import Path
import streamlit as st
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences

# â”€â”€ page config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="GRU Nextâ€‘Word Predictor",
    page_icon="ğŸ“",
    layout="wide",
    initial_sidebar_state="expanded",
)

ACCENT = "#7E3FF2"                     # purple accent colour

# â”€â”€ load artefacts once â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_FILE = "1_GRU_100epochs_without_ES.h5"   # adjust if needed
TOKENIZER_FILE = "tokenizer.pickle"

@st.cache_resource(show_spinner="Loading modelâ€¦")
def load_artefacts():
    mdl = load_model(MODEL_FILE)
    tok = pickle.loads(Path(TOKENIZER_FILE).read_bytes())
    return mdl, tok, mdl.input_shape[1] + 1

model, tokenizer, MAX_LEN = load_artefacts()

# â”€â”€ core inference â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def predict_next_word(text: str) -> str | None:
    seq = tokenizer.texts_to_sequences([text])[0]
    seq = seq[-(MAX_LEN - 1):]
    seq = pad_sequences([seq], maxlen=MAX_LEN - 1, padding="pre")
    idx = int(np.argmax(model.predict(seq, verbose=0), axis=1)[0])
    return tokenizer.index_word.get(idx, "(unknown)")

# â”€â”€ sidebar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.header("â„¹ï¸  About this demo")
    st.markdown(
        textwrap.dedent(f"""
        **Model**â€ƒGRUâ€ƒ|â€ƒ100â€¯epochsâ€ƒ|â€ƒ*no* EarlyÂ Stopping  
        **Training seqâ€‘len**â€ƒ{MAX_LEN-1} tokens  
        **Dataset**â€ƒYour custom corpus  
        **Latency**â€ƒâ‰ˆÂ 20â€¯ms on CPU  
        ---
        **How it works**  
        1. Text â†’ integer tokens via `tokenizer`  
        2. Sequence padded/truncated to {MAX_LEN-1}  
        3. GRU predicts probability for every word in vocab  
        4. Highestâ€‘probability index â†’ actual word
    """), help="This text lives in the sidebar")
    st.markdown("---")
    st.markdown("ğŸ’» *Made with StreamlitÂ 1.35*")
    st.markdown("[GitHubÂ repo](https://github.com/) â€¢ [HuggingÂ FaceÂ Space](https://huggingface.co/)")

# â”€â”€ main page header (â€œheroâ€) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown(
    f"""
    <h1 style='text-align:center; font-size:3rem;'>
    ğŸ“ Nextâ€‘Word Predictor
    </h1>
    <p style='text-align:center; color:{ACCENT}; font-size:1.1rem;'>
    GRUâ€‘based language model â€¢ trained 100Â epochs â€¢ no early stopping
    </p>
    """,
    unsafe_allow_html=True,
)

# â”€â”€ example picker + text input â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
examples = [
    "To be or not to",
    "Once upon a time",
    "Artificial intelligence is",
    "The quick brown fox",
    "In the beginning God",
]

left, right = st.columns([1, 3])
with left:
    choice = st.radio("Examples", examples, index=0, label_visibility="collapsed")
with right:
    user_text = st.text_input("Write or edit the prompt", value=choice, key="user_input")

st.markdown("")

# â”€â”€ predict button / slim result card â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if st.button("ğŸ”®  Predict next word", type="primary"):
    next_word = predict_next_word(user_text.strip())

    # label sits outside the card so the card can stay tiny
    st.caption("Predicted word")

    st.markdown(
        f"""
        <div style='max-width: 320px;                        /* narrower */
                    margin: 0 auto;  
                    background: #F7F2FF;
                    padding: 0.75rem 1rem;                  /* less padding */
                    border-radius: 0.6rem;
                    border: 1px solid {ACCENT}33;
                    box-shadow: 0 2px 5px rgba(0,0,0,0.05);'>
            <span style='font-size: 1.4rem;                 /* smaller text */
                         font-weight: 600;
                         color: {ACCENT};
                         display: block;
                         text-align: center;'>
                {next_word}
            </span>
        </div>
        """,
        unsafe_allow_html=True,
    )
# â”€â”€ footer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown(
    "<hr style='margin-top:3rem;'><div style='text-align:center;'>"
    "Â©Â 2025 RajÂ Patel â€¢ Powered by TensorFlow + Streamlit"
    "</div>", unsafe_allow_html=True
)
